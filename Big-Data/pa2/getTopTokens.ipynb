{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Getting Top Tokens\n",
    "\n",
    "* Use a sample set of (key, values) in order to figure this damn thing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../spark_variables.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sample Dataset\n",
    "\n",
    "* Something simpler than the tweets\n",
    "* Mimick what comes out of the tokenizer as a SET {  }\n",
    "* The set will automatically remove duplicate tokens / words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('111', {'a', 'b', 'c'}),\n",
       " ('111', {'a', 'b', 'd'}),\n",
       " ('222', {'a', 'b', 'e'}),\n",
       " ('333', {'a', 'd', 'g'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('111',{'a','b','c'}), ('111',{'a','b','d'}), ('222',{'a','b','e'}),('333',{'a','d','g'})])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new RRD that has the count of each token\n",
    "\n",
    "* each time the token appears per key, you count it once\n",
    "* when the token appears again for a second user, then you count it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = rdd.reduceByKey(lambda a,b: a|b)\\\n",
    "                    .flatMap(lambda x: ((x[0],y) for y in x[1]))\\\n",
    "                    .map(lambda x: (x[1],1))\\\n",
    "                    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 2), ('c', 1), ('b', 2), ('g', 1), ('a', 3), ('e', 1)]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(token_counts.collect())\n",
    "print(token_counts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the Top Tokens\n",
    "\n",
    "* Because this is a small dataset, we'll do >= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[('a', 3), ('d', 2), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "top_tokens = token_counts.filter(lambda x: x[1] >= 2).sortBy(lambda x: x[1],ascending=False).cache()\n",
    "\n",
    "frequent_tokens = top_tokens.count()\n",
    "\n",
    "top_x = top_tokens.take(5)\n",
    "\n",
    "print(frequent_tokens)\n",
    "print(top_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
