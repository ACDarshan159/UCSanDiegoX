{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important classes of Spark SQL and DataFrames:\n",
    "\n",
    "    - :class:`pyspark.sql.SQLContext`\n",
    "      Main entry point for :class:`DataFrame` and SQL functionality.\n",
    "    - :class:`pyspark.sql.DataFrame`\n",
    "      A distributed collection of data grouped into named columns.\n",
    "    - :class:`pyspark.sql.Column`\n",
    "      A column expression in a :class:`DataFrame`.\n",
    "    - :class:`pyspark.sql.Row`\n",
    "      A row of data in a :class:`DataFrame`.\n",
    "    - :class:`pyspark.sql.HiveContext`\n",
    "      Main entry point for accessing data stored in Apache Hive.\n",
    "    - :class:`pyspark.sql.GroupedData`\n",
    "      Aggregation methods, returned by :func:`DataFrame.groupBy`.\n",
    "    - :class:`pyspark.sql.DataFrameNaFunctions`\n",
    "      Methods for handling missing data (null values).\n",
    "    - :class:`pyspark.sql.DataFrameStatFunctions`\n",
    "      Methods for statistics functionality.\n",
    "    - :class:`pyspark.sql.functions`\n",
    "      List of built-in functions available for :class:`DataFrame`.\n",
    "    - :class:`pyspark.sql.types`\n",
    "      List of data types available.\n",
    "    - :class:`pyspark.sql.Window`\n",
    "      For working with window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add / Run this cell first to fix the issue with environment variables for Java not being set properly\n",
    "# Even though they're in .bash_profile\n",
    "# TODO: Fix this\n",
    "\n",
    "%run '../../spark_variables.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\") \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataframeStatFunctions\n",
    "\n",
    "Methods for statistics functionality. [documented here](http://takwatanabe.me/pyspark/generated/generated/pyspark.sql.DataFrameStatFunctions.html)\n",
    "\n",
    "* **approxQuantile(col, probabilities, relativeError)**\tCalculates the approximate quantiles of a numerical column of a DataFrame.\n",
    "* **corr(col1, col2[, method])**\tCalculates the correlation of two columns of a DataFrame as a double value.\n",
    "* **cov(col1, col2)**\tCalculate the sample covariance for the given columns, specified by their names, as a double value.\n",
    "* **crosstab(col1, col2)**\tComputes a pair-wise frequency table of the given columns.\n",
    "* **freqItems(cols[, support])**\tFinding frequent items for columns, possibly with false positives.\n",
    "* **sampleBy(col, fractions[, seed])**\tReturns a stratified sample without replacement based on the fraction given on each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrelativeError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculates the approximate quantiles of numerical columns of a\n",
       "DataFrame.\n",
       "\n",
       "The result of this algorithm has the following deterministic bound:\n",
       "If the DataFrame has N elements and if we request the quantile at\n",
       "probability `p` up to error `err`, then the algorithm will return\n",
       "a sample `x` from the DataFrame so that the *exact* rank of `x` is\n",
       "close to (p * N). More precisely,\n",
       "\n",
       "  floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
       "\n",
       "This method implements a variation of the Greenwald-Khanna\n",
       "algorithm (with some speed optimizations). The algorithm was first\n",
       "present in [[http://dx.doi.org/10.1145/375663.375670\n",
       "Space-efficient Online Computation of Quantile Summaries]]\n",
       "by Greenwald and Khanna.\n",
       "\n",
       "Note that null values will be ignored in numerical columns before calculation.\n",
       "For columns only containing null values, an empty list is returned.\n",
       "\n",
       ":param col: str, list.\n",
       "  Can be a single column name, or a list of names for multiple columns.\n",
       ":param probabilities: a list of quantile probabilities\n",
       "  Each number must belong to [0, 1].\n",
       "  For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
       ":param relativeError:  The relative target precision to achieve\n",
       "  (>= 0). If set to zero, the exact quantiles are computed, which\n",
       "  could be very expensive. Note that values greater than 1 are\n",
       "  accepted but give the same result as 1.\n",
       ":return:  the approximate quantiles at the given probabilities. If\n",
       "  the input `col` is a string, the output is a list of floats. If the\n",
       "  input `col` is a list or tuple of strings, the output is also a\n",
       "  list, but each element in it is a list of floats, i.e., the output\n",
       "  is a list of list of floats.\n",
       "\n",
       ".. versionchanged:: 2.2\n",
       "   Added support for multiple columns.\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.approxQuantile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculates the correlation of two columns of a DataFrame as a double value.\n",
       "Currently only supports the Pearson Correlation Coefficient.\n",
       ":func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
       "\n",
       ":param col1: The name of the first column\n",
       ":param col2: The name of the second column\n",
       ":param method: The correlation method. Currently only supports \"pearson\"\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.corr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculate the sample covariance for the given columns, specified by their names, as a\n",
       "double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
       "\n",
       ":param col1: The name of the first column\n",
       ":param col2: The name of the second column\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.cov?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
       "table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
       "non-zero pair frequencies will be returned.\n",
       "The first column of each row will be the distinct values of `col1` and the column names\n",
       "will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
       "Pairs that have no occurrences will have zero as their counts.\n",
       ":func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
       "\n",
       ":param col1: The name of the first column. Distinct items will make the first item of\n",
       "    each row.\n",
       ":param col2: The name of the second column. Distinct items will make the column names\n",
       "    of the DataFrame.\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.crosstab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Finding frequent items for columns, possibly with false positives. Using the\n",
       "frequent element count algorithm described in\n",
       "\"http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
       ":func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
       "\n",
       ".. note:: This function is meant for exploratory data analysis, as we make no\n",
       "    guarantee about the backward compatibility of the schema of the resulting DataFrame.\n",
       "\n",
       ":param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
       "    strings.\n",
       ":param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
       "    The support must be greater than 1e-4.\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.freqItems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a stratified sample without replacement based on the\n",
       "fraction given on each stratum.\n",
       "\n",
       ":param col: column that defines strata\n",
       ":param fractions:\n",
       "    sampling fraction for each stratum. If a stratum is not\n",
       "    specified, we treat its fraction as zero.\n",
       ":param seed: random seed\n",
       ":return: a new DataFrame that represents the stratified sample\n",
       "\n",
       ">>> from pyspark.sql.functions import col\n",
       ">>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
       ">>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
       ">>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
       "+---+-----+\n",
       "|key|count|\n",
       "+---+-----+\n",
       "|  0|    5|\n",
       "|  1|    9|\n",
       "+---+-----+\n",
       "\n",
       ".. versionadded:: 1.5\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataFrameStatFunctions.sampleBy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "64px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
